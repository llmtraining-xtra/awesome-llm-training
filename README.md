<body>
<div><h1>awesome-llm-training</h1>

<div id="__source__">
<h2>Automatic Parallelism Configuration System</h2>
<table>
<tr>
    <th>Content</th>
    <th>Source</th>
</tr>
<tr>
    <td>
        Aceso: Efficient Parallel DNN Training through Iterative Bottleneck Alleviation [Eurosys'24]
    </td>
    <td>
        [<a href="https://dl.acm.org/doi/pdf/10.1145/3627703.3629554">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization [OSDI'22]
    </td>
    <td>
        [<a href="https://www.usenix.org/system/files/osdi22-unger.pdf">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning [OSDI'22]
    </td>
    <td>
        [<a href="https://www.usenix.org/system/files/osdi22-zheng-lianmin.pdf">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        Whale: Efficient Giant Model Training over Heterogeneous GPUs [ATC'22]
    </td>
    <td>
        [<a href="https://www.usenix.org/system/files/atc22-jia-xianyan.pdf">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        Piper: Multidimensional Planner for DNN Parallelization [NIPS'21]
    </td>
    <td>
        [<a href="https://proceedings.neurips.cc/paper/2021/file/d01eeca8b24321cd2fe89dd85b9beb51-Paper.pdf">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        DAPPLE: A Pipelined Data Parallel Approach for Training Large Models [PPoPP'21]
    </td>
    <td>
        [<a href="https://arxiv.org/pdf/2007.01045">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        Supporting Very Large Models using Automatic Dataflow Graph Partitioning [Eurosys'19]
    </td>
    <td>
        [<a href="https://dl.acm.org/doi/pdf/10.1145/3302424.3303953">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        PipeDream: generalized pipeline parallelism for DNN training [SOSP'19]
    </td>
    <td>
        [<a href="https://par.nsf.gov/servlets/purl/10129641">Paper</a>]
    </td>
</tr>
<tr>
    <td>
        Beyond Data and Model Parallelism for Deep Neural Networks [MLSys'19]
    </td>
    <td>
        [<a href="https://proceedings.mlsys.org/paper_files/paper/2019/file/b422680f3db0986ddd7f8f126baaf0fa-Paper.pdf">Paper</a>]
    </td>
<<tr>
<td>
    大模型分布式训练并行技术 by 吃果冻不吐果冻皮 (zhihu)  
</td>
<td>
    [<a href="https://zhuanlan.zhihu.com/p/598714869">Posts</a>]
    [<a href="https://github.com/liguodongiot/llm-action">Repo</a>]
</td>
</tr>
/tr>
</table>


<h2>Recommendation System</h2>
<table>
<tr>
    <th>Paper</th>
    <th>Source</th>
</tr>
<tr>
    <td>
        Transparent GPU Sharing in Container Clouds for Deep Learning Workloads [NSDI'23]
    </td>
    <td>
        [<a href="https://www.usenix.org/conference/nsdi23/presentation/wu">Paper</a>]
        [<a href="https://github.com/pkusys/TGS">Code</a>]
    </td>
</tr>
</table>

</div>

</div>
</body>
